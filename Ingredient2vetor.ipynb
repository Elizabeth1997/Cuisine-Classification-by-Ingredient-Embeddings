{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ingredient2vetor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8qin2A9qVGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import json\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIzuTF9TGNU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "# run on the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DataReader:\n",
        "    NEGATIVE_TABLE_SIZE = 1e8\n",
        "\n",
        "    def __init__(self, inputFileName, min_count):\n",
        "\n",
        "        self.negatives = []\n",
        "        self.discards = []\n",
        "        self.negpos = 0\n",
        "\n",
        "        self.word2id = dict()\n",
        "        self.id2word = dict()\n",
        "        self.sentences_count = 0\n",
        "        self.token_count = 0\n",
        "        self.word_frequency = dict()\n",
        "\n",
        "        self.inputFileName = inputFileName\n",
        "        self.read_words(min_count)\n",
        "        self.initTableNegatives()\n",
        "        self.initTableDiscards()\n",
        "        \n",
        "    def plot_frequency(self):\n",
        "        pass\n",
        "      \n",
        "    def read_words(self, min_count):\n",
        "        word_frequency = dict()\n",
        "        for line in open(self.inputFileName, encoding=\"utf8\"):\n",
        "            line = line.split()\n",
        "            if len(line) > 1:\n",
        "                self.sentences_count += 1\n",
        "                for word in line:\n",
        "                    if len(word) > 0:\n",
        "                        self.token_count += 1\n",
        "                        word_frequency[word] = word_frequency.get(word, 0) + 1\n",
        "\n",
        "                        if self.token_count % 1000000 == 0:\n",
        "                            print(\"Read \" + str(int(self.token_count / 1000000)) + \"M words.\")\n",
        "        # show each word's frequency before the discard action\n",
        "#         plot_frequency()\n",
        "        wid = 0\n",
        "        print()\n",
        "        # w represents the word; c is the frequency of the word\n",
        "        for w, c in word_frequency.items():\n",
        "            if c < min_count:\n",
        "                continue\n",
        "            # if the counts of one word is less than min_count, then don't put this word in the vocabulary\n",
        "            self.word2id[w] = wid\n",
        "            self.id2word[wid] = w\n",
        "            self.word_frequency[wid] = c\n",
        "            wid += 1\n",
        "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
        "\n",
        "    def initTableDiscards(self):\n",
        "        t = 0.00001\n",
        "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
        "        # every ingredient's Probability to be discarded\n",
        "        self.discards = np.sqrt(t / f) + (t / f)\n",
        "\n",
        "    def initTableNegatives(self):\n",
        "        pow_frequency = np.array(list(self.word_frequency.values())) ** 0.5\n",
        "        words_pow = sum(pow_frequency)\n",
        "        ratio = pow_frequency / words_pow\n",
        "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
        "        for wid, c in enumerate(count):\n",
        "            self.negatives += [wid] * int(c)\n",
        "        self.negatives = np.array(self.negatives)\n",
        "        np.random.shuffle(self.negatives)\n",
        "\n",
        "    def getNegatives(self, target, size):  # TODO check equality with target\n",
        "        response = self.negatives[self.negpos:self.negpos + size]\n",
        "        self.negpos = (self.negpos + size) % len(self.negatives)\n",
        "        if len(response) != size:\n",
        "            return np.concatenate((response, self.negatives[0:self.negpos]))\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1zjOzvTHy0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class Word2vecDataset(Dataset):\n",
        "  # data is the object of class DataReader\n",
        "    def __init__(self, data, window_size):\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.input_file = open(data.inputFileName, encoding=\"utf8\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.sentences_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            line = self.input_file.readline()\n",
        "            if not line:\n",
        "                self.input_file.seek(0, 0)\n",
        "                line = self.input_file.readline()\n",
        "\n",
        "            if len(line) > 1:\n",
        "                words = line.split()\n",
        "\n",
        "                if len(words) > 1:\n",
        "                    word_ids = [self.data.word2id[w] for w in words if\n",
        "                                # according to the discard probabilty to decide keep this word or not so called: subsampling\n",
        "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
        "\n",
        "                    boundary = np.random.randint(1, self.window_size)\n",
        "                    # negative sampling\n",
        "                    return [(u, v, self.data.getNegatives(v, 5)) for i, u in enumerate(word_ids) for j, v in\n",
        "                            enumerate(word_ids[max(i - boundary, 0):i + boundary]) if u != v]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        # u - center word\n",
        "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
        "        # v - neighbor words\n",
        "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
        "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
        "\n",
        "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4bbWYLqKclm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# food2vector = drive.CreateFile({'id':'17OhgD4U6nOza3Tbt7l76HRaQCllESIBj'})\n",
        "# food2vector.GetContentFile('food2vec.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q28r3zM7G2rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "    u_embedding: Embedding for center word.\n",
        "    v_embedding: Embedding for neighbor words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "\n",
        "        initrange = 1.0 / self.emb_dimension\n",
        "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
        "        init.constant_(self.v_embeddings.weight.data, 0)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        emb_neg_v = self.v_embeddings(neg_v)\n",
        "\n",
        "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
        "        score = torch.clamp(score, max=10, min=-10)\n",
        "        score = -F.logsigmoid(score)\n",
        "\n",
        "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
        "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
        "\n",
        "        return torch.mean(score + neg_score)\n",
        "\n",
        "    def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        with open(file_name, 'w') as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            csv_writer.writerow(['Ingredient', 'Vector'])\n",
        "            for wid, w in id2word.items():\n",
        "                vector = str(list(embedding[wid]))\n",
        "                csv_writer.writerow([w,vector])\n",
        "        return embedding, id2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii5n2F8sG9vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from data_reader import DataReader, Word2vecDataset\n",
        "# from model import SkipGramModel\n",
        "\n",
        "\n",
        "class Word2VecTrainer:\n",
        "    def __init__(self, input_file, output_file, emb_dimension=100, batch_size=32, window_size=5, iterations=10,\n",
        "                 initial_lr=0.001, min_count=5):\n",
        "\n",
        "        self.data = DataReader(input_file, min_count)\n",
        "        # dataset is the object of class Word2vecDataset\n",
        "        dataset = Word2vecDataset(self.data, window_size)\n",
        "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                     shuffle=False, num_workers=0, collate_fn=dataset.collate)\n",
        "\n",
        "        self.output_file_name = output_file\n",
        "        self.emb_size = len(self.data.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.iterations = iterations\n",
        "        self.initial_lr = initial_lr\n",
        "        # put model on the GPU\n",
        "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension).to(device)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "\n",
        "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
        "            optimizer = optim.SparseAdam(self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
        "\n",
        "                if len(sample_batched[0]) > 1:\n",
        "                    # put training data on the GPU\n",
        "                    pos_u = sample_batched[0].to(device)\n",
        "                    pos_v = sample_batched[1].to(device)\n",
        "                    neg_v = sample_batched[2].to(device)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "                    loss.backward()\n",
        "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
        "                    if i > 0 and i % 500 == 0:\n",
        "                        print(\" Loss: \" + str(running_loss))\n",
        "        return self.skip_gram_model\n",
        "#     u_embeddings.weight, data.word2id, data.id2word\n",
        "#         return self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGNckq5dkiCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "class CuisinePredictDataset(Dataset):\n",
        "  # data is the object of class DataReader\n",
        "    def __init__(self, inputFileName, targetFileName, w2v):\n",
        "        # one-hot representation\n",
        "        self.target_array = self.get_target(targetFileName)\n",
        "        self.input_file = open(inputFileName, encoding=\"utf8\")\n",
        "        self.food2id = w2v.data.word2id\n",
        "        self.id2food = w2v.data.id2word\n",
        "        self.input_data = []\n",
        "        self.max_num_gredients = 0\n",
        "        # the total number of cuisine's categories\n",
        "#         self.cuisine_types = 0\n",
        "        # get the input_data using the method below\n",
        "        self.get_padded_ingredients_ids()\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of training data\n",
        "        return len(self.target_array)\n",
        "    \n",
        "    def get_target(self, target_file_path):\n",
        "        cuisine_data = pd.read_csv(target_file_path)\n",
        "        self.cuisine_types = int(cuisine_data.nunique())\n",
        "        print(self.cuisine_types)\n",
        "        cuisine_array = cuisine_data.to_numpy()\n",
        "        # label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        integer_encoded = label_encoder.fit_transform(cuisine_array.ravel())\n",
        "        # binary encode\n",
        "        onehot_encoder = OneHotEncoder(categories='auto',sparse=False)\n",
        "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "        # integer encoded target\n",
        "        self.target_integer_encoded = integer_encoded\n",
        "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "        return onehot_encoded\n",
        "#         return torch.from_numpy(onehot_encoded) # convert to tensor object\n",
        "\n",
        "    def get_padded_ingredients_ids(self):\n",
        "        while True:\n",
        "            line = self.input_file.readline()\n",
        "            # not the last line of the file\n",
        "            if line:\n",
        "                ingredients_list = line.split()\n",
        "                # the number of ingredients would be changed, so 30 cannot be used\n",
        "                ingredients_ids = [self.food2id[ingre] for ingre in ingredients_list if ingre in self.food2id]\n",
        "                self.input_data.append(ingredients_ids)\n",
        "                if self.max_num_gredients < len(ingredients_ids):\n",
        "                    self.max_num_gredients = len(ingredients_ids)\n",
        "            else:\n",
        "                break\n",
        "#         max_length = self.max_num_gredients\n",
        "        print('The maximum number of the ingredients in each recipe is {}'.format(self.max_num_gredients))\n",
        "        max_index = max(self.id2food.keys())\n",
        "        self.input_data = [ingredient + [max_index]*(self.max_num_gredients - len(ingredient)) for ingredient in self.input_data]\n",
        "                          \n",
        "             \n",
        "    def __getitem__(self, idx):\n",
        "#         return self.input_data[idx], self.target_array.tolist()[idx]\n",
        "        # get one training sample based on the index\n",
        "        return [self.input_data[idx],self.target_array.tolist()[idx],self.target_integer_encoded.tolist()[idx]]\n",
        "        \n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        # ingredients\n",
        "        all_ingredients = [batch[0] for batch in batches]\n",
        "        # cusine labels one hot\n",
        "        all_cusine_labels = [batch[1] for batch in batches]\n",
        "        # cuisine labels integer encoded\n",
        "        all_cusine_integer_encoded = [batch[2] for batch in batches]\n",
        "        return torch.LongTensor(all_ingredients), torch.LongTensor(all_cusine_labels), torch.LongTensor(all_cusine_integer_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EP0e8VKkm3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after get the food2vector, do cuisine classification\n",
        "class CuisineModel(nn.Module):\n",
        "    def __init__(self, embedding_weight, num_class):\n",
        "        super().__init__()\n",
        "        self.vocab_size, self.embed_dim = embedding_weight.shape\n",
        "        # padding the last row\n",
        "        zero_plus = torch.zeros(1,self.embed_dim)\n",
        "        self.embedding_weight = torch.cat((embedding_weight, zero_plus),dim=0)\n",
        "        # embedding layer, don't update but when the weight updates the performance of the model is pretty bad!\n",
        "        # freeze the embedding weight -> better result\n",
        "        self.embedding = nn.EmbeddingBag.from_pretrained(self.embedding_weight, freeze=False, mode='mean')\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(self.embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "        # the active function of the last layer: softmax\n",
        "        self.soft_max = nn.Softmax(dim=1)\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        # iunitialize the weights of fcl\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, ingredients_indexes):\n",
        "        ingredients_embedding = self.embedding(ingredients_indexes)\n",
        "        ingredients_fc = self.fc(ingredients_embedding) \n",
        "        return self.soft_max(ingredients_fc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0jC1pcxoLRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CuisineModelCNN(nn.Module):\n",
        "  \n",
        "    def __init__(self, embedding_weight, num_class, batch_size):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size, self.embed_dim = embedding_weight.shape\n",
        "        zero_plus = torch.zeros(1,self.embed_dim)\n",
        "        self.embedding_weight = torch.cat((embedding_weight, zero_plus),dim=0)\n",
        "        # size should be 20 by 100\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.embedding_weight,freeze=True) \n",
        "#         self.embedding = nn.Embedding(self.vocab_size + 1, self.embed_dim,sparse=True)\n",
        "        # static cnn: the height of kernel doesn't change\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,self.embed_dim), stride=1, padding=0)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=18, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
        "#         self.maxpool2d = nn.MaxPool2d(2, stride=2)\n",
        "        self.maxpool1d = nn.MaxPool1d(18)\n",
        "        # fully connected layer\n",
        "        self.fc1 = nn.Linear(8, num_class)\n",
        "#         self.fc2 = nn.Linear(16, num_class)\n",
        "#         self.fc2 = nn.Linear(1024, num_class)\n",
        "#         self.init_weights()\n",
        "        # the active function of the last layer: softmax\n",
        "        self.soft_max = nn.Softmax(dim=1)\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         initrange = 0.5\n",
        "#         # iunitialize the weights of fcl\n",
        "#         self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "#         self.fc1.bias.data.zero_()\n",
        "# #         self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "# #         self.fc2.bias.data.zero_()\n",
        "\n",
        "    def forward(self, ingredients_indexes):\n",
        "        ingredients_embedding = self.embedding(ingredients_indexes)\n",
        "        # reshape\n",
        "        ingredients_embedding = ingredients_embedding.view(-1, 1,20,100)\n",
        "#         ingredients_embedding = torch.reshape(ingredients_embedding, (1, 1, 20, 100))\n",
        "        x = self.conv1(ingredients_embedding) # 32 18 18 1\n",
        "#         x = torch.reshape(x, (1, 1, 18, 18))\n",
        "#         x = self.conv2(x)\n",
        "#         x = x.view(-1, 1,18,18)\n",
        "#         x = self.conv2(x) # 32 1 16 16\n",
        "#         x = self.maxpool2d(x) # 32 1 8 8 \n",
        "#         print(x.shape) torch.Size([32, 1, 8, 8])\n",
        "#         print(x.view(-1,18).shape) [32,18]\n",
        "        x = x.view(-1,8,18)\n",
        "        x = self.maxpool1d(x)\n",
        "        x = self.fc1(x.view(-1,8))\n",
        "#         ingredients_fc = self.fc2(x)\n",
        "#         print(ingredients_fc.shape) torch.Size([32, 20])\n",
        "        return self.soft_max(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrPJJkwBkvbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import statistics\n",
        "from google.colab import files\n",
        "\n",
        "def train_cuisine_model(train_data, validation_data, model, criterion, optimizer, batch_size, save_path):\n",
        "    since = time.time()\n",
        "    loss_list = []\n",
        "    corrects_list = []\n",
        "   \n",
        "    for i, (ingredients_indexes, cuisine_label, cuisine_label_in) in enumerate(train_data):\n",
        "            optimizer.zero_grad()\n",
        "            ingredients_indexes, cuisine_label, cuisine_label_in = ingredients_indexes.to(device), cuisine_label.to(device), cuisine_label_in.to(device)\n",
        "            outputs = model(ingredients_indexes)\n",
        "            batch_size, _ = outputs.shape\n",
        "            # the target is supposed to be 1-d\n",
        "            loss = criterion(outputs, cuisine_label_in.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicts = torch.max(outputs, 1)\n",
        "            if i %  29== 0:\n",
        "              running_loss = loss.item()\n",
        "              running_corrects = torch.sum(predicts == cuisine_label_in.squeeze()).float()\n",
        "              print('Iteration time: {} loss: {:.3f} accuracy: {: .2f}'.format(i+1, running_loss, running_corrects / batch_size))\n",
        "              loss_list.append(running_loss)\n",
        "              corrects_list.append(running_corrects)\n",
        "  #               break # debug\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print('Model has been saved successfully in {}.'.format(save_path))\n",
        "#     files.download(save_path) \n",
        "    # test\n",
        "#     total_accuracy = []\n",
        "#     for j, (ingredients_indexes, cuisine_label, cuisine_label_in) in enumerate(validation_data):\n",
        "#       ingredients_indexes, cuisine_label_in = ingredients_indexes.to(device), cuisine_label_in.to(device)\n",
        "#       with torch.no_grad():\n",
        "#         outputs = model(ingredients_indexes)\n",
        "#         _, predicts = torch.max(outputs, 1)\n",
        "#         accuracy = torch.sum(predicts == cuisine_label_in.squeeze()).float() / batch_size\n",
        "#         total_accuracy.append(accuracy)\n",
        "#         if j % 29 == 0:\n",
        "#           print('Iteration time: {} accuracy: {: .2f}'.format(j+1, accuracy))\n",
        "# #           break # debug\n",
        "#     # j represents the total number of batches\n",
        "#     print('Validation accuracy is {:.3f}'.format(sum(total_accuracy)/(j+1)))\n",
        "    return model, loss_list, corrects_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5nbFDUxmC0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJLLigPom0aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuisine_target = drive.CreateFile({'id':'1HJYeL_lSseHqh94Lo5EvOPayhSOFKMrQ'})\n",
        "cuisine_target.GetContentFile('cuisine_target.csv')\n",
        "# https://drive.google.com/open?id=1HJYeL_lSseHqh94Lo5EvOPayhSOFKMrQ\n",
        "ingredients = drive.CreateFile({'id':'1ZiaYobyIm00GhxFdcgtuw7P1BSFQXBli'})\n",
        "ingredients.GetContentFile('ingredients.txt')\n",
        "# https://drive.google.com/open?id="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qT9eKO_0vUx",
        "colab_type": "text"
      },
      "source": [
        "Use original data instead of processed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HiOQ7NP0Ng_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_cuisine_target = drive.CreateFile({'id':'1E4y5JuLQpeqlbRCxef3eOt0sZRjQdC-E'})\n",
        "original_cuisine_target.GetContentFile('original_cuisine_target.csv')\n",
        "# https://drive.google.com/open?id=1E4y5JuLQpeqlbRCxef3eOt0sZRjQdC-E\n",
        "original_ingredients = drive.CreateFile({'id':'18fbpzRaAwvsieVch0ADiHqg7mtJggzaP'})\n",
        "original_ingredients.GetContentFile('original_ingredients.txt')\n",
        "# https://drive.google.com/open?id=18fbpzRaAwvsieVch0ADiHqg7mtJggzaP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFM4Xl2Y70n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "targetFileName = 'cuisine_target.csv'\n",
        "inputFileName = 'ingredients.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GmDK5GTkw9a",
        "colab_type": "code",
        "outputId": "fe42de0c-1e7c-48f6-fe6b-2272228d51a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "raw_targetFileName = 'original_cuisine_target.csv'\n",
        "raw_inputFileName = 'original_ingredients.txt'\n",
        "# w2v = Word2VecTrainer(input_file=raw_inputFileName, output_file=\"food2vec.csv\") # Total embeddings: 3337\n",
        "w2v = Word2VecTrainer(input_file=inputFileName, output_file=\"food2vec.csv\") # Total embeddings: 2332\n",
        "skip_gram_model = w2v.train()\n",
        "# the embedding size is 3257 by 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total embeddings: 3257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1205 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 541/1205 [00:02<00:03, 211.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.113688501556179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1025/1205 [00:05<00:00, 210.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.961316169195889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:06<00:00, 200.24it/s]\n",
            "  2%|▏         | 22/1205 [00:00<00:05, 213.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 526/1205 [00:02<00:03, 213.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.626473694863305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1030/1205 [00:04<00:00, 205.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4560466094762736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 211.41it/s]\n",
            "  2%|▏         | 19/1205 [00:00<00:06, 183.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 543/1205 [00:02<00:03, 209.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.2132203954147878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 1023/1205 [00:04<00:00, 209.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.1855466239184134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 209.07it/s]\n",
            "  2%|▏         | 21/1205 [00:00<00:05, 208.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 530/1205 [00:02<00:03, 208.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.9940048089776434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 1038/1205 [00:04<00:00, 212.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.9978273762330074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 210.35it/s]\n",
            "  2%|▏         | 22/1205 [00:00<00:05, 212.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 541/1205 [00:02<00:03, 214.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8729544364083566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1029/1205 [00:04<00:00, 213.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8946842429742965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 211.38it/s]\n",
            "  2%|▏         | 23/1205 [00:00<00:05, 221.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 527/1205 [00:02<00:03, 213.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.788097362627227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 1034/1205 [00:04<00:00, 213.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8509355317018614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 212.42it/s]\n",
            "  2%|▏         | 23/1205 [00:00<00:05, 221.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 527/1205 [00:02<00:03, 215.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.762638279150128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 1034/1205 [00:04<00:00, 211.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.8001600524059493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 212.54it/s]\n",
            "  2%|▏         | 22/1205 [00:00<00:05, 214.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 529/1205 [00:02<00:03, 212.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.7448284428717074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 1038/1205 [00:04<00:00, 216.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.7919773767416447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 215.02it/s]\n",
            "  2%|▏         | 23/1205 [00:00<00:05, 225.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 527/1205 [00:02<00:03, 214.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.7200831933248666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 1032/1205 [00:04<00:00, 212.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.7741753168438525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 213.08it/s]\n",
            "  2%|▏         | 22/1205 [00:00<00:05, 207.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 544/1205 [00:02<00:03, 212.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.699772932827364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1028/1205 [00:04<00:00, 210.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 2.740092924537403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1205/1205 [00:05<00:00, 210.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUT5Ila7qiQl",
        "colab_type": "code",
        "outputId": "bf03b4eb-eedd-47db-a419-ac9ea9c6f7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# batch_size = 12 needs 70 mins to finised the training process\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# num_class = dataset.cuisine_types\n",
        "dataset = CuisinePredictDataset(inputFileName=inputFileName, targetFileName=targetFileName, w2v=w2v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "The maximum number of the ingredients in each recipe is 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJtz9UXIYWm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # ratio 7:3 split dataset into 7 portions for training and the rest of 3 for testing\n",
        "split_ratio = 0.8\n",
        "train_len = int(len(dataset) * split_ratio)\n",
        "train_data, validation_data = random_split(dataset, [train_len, len(dataset)-train_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxh51KUgZtR4",
        "colab_type": "code",
        "outputId": "a4b8770f-0a19-45c7-b352-9889bcc1cc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " print('{} samples for training and {} samples for validation'.format(len(train_data),len(validation_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30844 samples for training and 7711 samples for validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5oh-NjS5hsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(skip_gram_model.u_embeddings.weight.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UELc_pWOytSk",
        "colab_type": "text"
      },
      "source": [
        "Fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjwaobvZqzY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cuisine_model = CuisineModel(embedding_weight=skip_gram_model.u_embeddings.weight.cpu(), num_class=dataset.cuisine_types).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqyr8b7_R_-F",
        "colab_type": "code",
        "outputId": "88ccc091-c5b7-4248-c2c1-daaaa3013b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "cuisine_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CuisineModel(\n",
              "  (embedding): EmbeddingBag(3258, 100, mode=mean)\n",
              "  (fc): Linear(in_features=100, out_features=20, bias=True)\n",
              "  (soft_max): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVf-XFdmyxOM",
        "colab_type": "text"
      },
      "source": [
        "Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEvrEayqyqKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuisine_model_cnn = CuisineModelCNN(embedding_weight=skip_gram_model.u_embeddings.weight.cpu(), batch_size = batch_size, num_class=dataset.cuisine_types).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlbYM4jpzNVK",
        "colab_type": "code",
        "outputId": "2280f21e-71d6-4bed-c13c-198f0e73a678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "cuisine_model_cnn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CuisineModelCNN(\n",
              "  (embedding): Embedding(3258, 100)\n",
              "  (conv1): Conv2d(1, 8, kernel_size=(3, 100), stride=(1, 1))\n",
              "  (maxpool1d): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=8, out_features=20, bias=True)\n",
              "  (soft_max): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUtNrYNOx0Lc",
        "colab_type": "code",
        "outputId": "df62f422-c4e6-4bbe-e10e-a826989eb142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# dataset.input_data[1]\n",
        "len(dataset.input_data) #38555 is the true number of training samples; while the length of input_data is 19278, which is apparently incorrect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fsQ3TlxTJQzN",
        "colab": {}
      },
      "source": [
        "# SGD as the optimizer\n",
        "optimizer = torch.optim.SGD(cuisine_model_cnn.parameters(), lr=learning_rate, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_deA3Xgtu-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDmTloVXZXZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = DataLoader(validation_data, batch_size=batch_size, collate_fn=dataset.collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYP89ouw3By8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path='cuisine_weight_cnn_nopretrain.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XttdM2DetxTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# why list out of range? how does the __item__ function work?\n",
        "cuisine_model_cnn, loss, corrects = train_cuisine_model(train_data, validation_data, cuisine_model_cnn, criterion, optimizer, batch_size=batch_size, save_path=save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYAAmrD12Etm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(save_path) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNh3VIQlHU7P",
        "colab_type": "code",
        "outputId": "141fbfe8-2f4c-4dec-8eec-4f95b03a1ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cuisine_model_cnn.load_state_dict(torch.load(save_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_IQDk2nJOdZ",
        "colab_type": "code",
        "outputId": "fed7c53b-0e4d-4219-9ec3-51d96bc8ccf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "cuisine_model_cnn.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CuisineModelCNN(\n",
              "  (embedding): Embedding(2333, 100)\n",
              "  (conv1): Conv2d(1, 8, kernel_size=(3, 100), stride=(1, 1))\n",
              "  (conv2): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (maxpool1d): MaxPool1d(kernel_size=18, stride=18, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=8, out_features=20, bias=True)\n",
              "  (soft_max): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0yemfv_CVq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = cuisine_model_cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq0ZoyaCH_eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_accuracy = []\n",
        "for j, (ingredients_indexes, cuisine_label, cuisine_label_in) in enumerate(validation_data):\n",
        "      ingredients_indexes, cuisine_label_in = ingredients_indexes.to(device), cuisine_label_in.to(device)\n",
        "      with torch.no_grad():\n",
        "        outputs = model(ingredients_indexes)\n",
        "        _, predicts = torch.max(outputs, 1)\n",
        "        accuracy = torch.sum(predicts == cuisine_label_in.squeeze()).float() / batch_size\n",
        "        total_accuracy.append(accuracy)\n",
        "        if j % 29 == 0:\n",
        "          print('Iteration time: {} accuracy: {: .2f}'.format(j+1, accuracy))\n",
        "    # j represents the total number of batches\n",
        "print('Validation accuracy is {:.3f}'.format(sum(total_accuracy)/(j+1)))\n",
        "# fully connected layer 23% \n",
        "# Validation accuracy is 1.3% when update the embedding\n",
        "# cnn 38.1%\n",
        "# cnn iteration times: 10 count<10  53.1%\n",
        "# cnn iteration times: 20 batch_size=16 52.3%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw_wa3BpVOps",
        "colab_type": "text"
      },
      "source": [
        "After get the food2vector, compare the result with webiste https://altosaar.github.io/food2vec/, make some progress to get better food2vec . using cnn，each user can be Regareded as a filter, the recipes he interacted can be formed as a channel map, try to predict the rating "
      ]
    }
  ]
}